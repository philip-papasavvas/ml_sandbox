{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "senior-allowance",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1\">Motivation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-descent\" data-toc-modified-id=\"Gradient-descent-1.1\">Gradient descent</a></span></li><li><span><a href=\"#Classification-&amp;-Representation\" data-toc-modified-id=\"Classification-&amp;-Representation-1.2\">Classification &amp; Representation</a></span></li><li><span><a href=\"#Overfitting-problem\" data-toc-modified-id=\"Overfitting-problem-1.3\">Overfitting problem</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-illinois",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "Having completed Professor Andrew Ng's Coursera [course](https://www.coursera.org/learn/machine-learning) on Machine Learning at the end of last year, I've decided to collate some of the key points from the course below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-dubai",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-penguin",
   "metadata": {},
   "source": [
    "In practice there are two steps that can take part to optimise the process:\n",
    "- **Feature Scaling**: ensure the features are on a similar scale before gradient decent takes place.\n",
    "    - Mean normalisation is replacing each individual value with its value minus the mean so that the features have approximately zero mean.\n",
    "    - One may choose to divide by the standard deviation to ensure the data has unit variance\n",
    "- **Learning rate**: to ensure gradient descent is learning correctly, ensure that the cost function is decreasing after each iteration. If not, the learning rate must be adjusted. If the cost function increases with each iteration, one may have to reduce the learning rate to that the cost function reaches a global minimum.\n",
    "    - Could use an automatic convergence test, whereby convergence is declared if the cost function decreases by less than $10^{-3}$ after each iteration.\n",
    "    - For sufficiently small learning rate, the cost function should decrease on each iteration, but if the larning rate is too small, gradient descent can be sow to converge\n",
    "    - If the learning rate is too large, the cost function may fail to decrease on each iteration, and therefore could fail to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-teacher",
   "metadata": {},
   "source": [
    "One might choose to approach the problem of solving for the optimal input features using the **normal equation**, solving for the solution analytically. Using this method, there is no need for feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-investigation",
   "metadata": {},
   "source": [
    "## Classification & Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-vulnerability",
   "metadata": {},
   "source": [
    "Use a classification algorithm where the label is a discrete value, for example classifying an email as spam/not spam, or if a tumour is malignant/benign.\n",
    "\n",
    "For a classification problem, the hypothesis is represented by a sigmoid/logistic function, that can take any value between 0 and 1. The hypothesis in this case is the estimated probability that $y$ (response variable) is 1 given an input $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-geneva",
   "metadata": {},
   "source": [
    "For a multi-class classification problem, it can be solved using the:\n",
    "- One versus all/one versus rest approach: split each different class into a binary problem. For example if there are four different, discrete classes 1 to 4. Split the problem into being:\n",
    "        - 1 versus 2, 3 and 4 (grouped)\n",
    "        - 2 versus 1, 3 and 4\n",
    "        - 3 versus 1, 2 and 4\n",
    "        - 4 versus 1, 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-relay",
   "metadata": {},
   "source": [
    "## Overfitting problem\n",
    "When trying to apply either linear or logistic regression to a problem, if you have too many features, the cost function may be zero, so the learned hypothesis fails to generalise for new examples. In this case, the model has **overfit**, learning so well on the old data, that it doesn't perform well on new examples. \n",
    "\n",
    "One doesn't want a function to either **underfit** (high bias) or **overfit** (high variance).\n",
    "\n",
    "In order to address overfitting:\n",
    "- Reduce the number of features: \n",
    "    1. Manually choose which features to keep\n",
    "    2. Use a model selection algorithm to choose the features to keep\n",
    "- Regularisation\n",
    "    1. Keep all features, but reduce the magnitude/values of the coeffiecients to zero. It will discourage learning a more complex or flexible model, to prevent overfitting.\n",
    "    2. Generally works well when there are lots of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
